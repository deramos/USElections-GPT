{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "814d98b1-5f48-44a5-a4b4-83aa5ca8be9c",
   "metadata": {},
   "source": [
    "# News Article Summarization. \n",
    "This notebook prepares news articles to be inserted into the ChromaDB vector database. First, it creates a summary of the news article, then it identifies important Named Entities such as the names of politicians, locations, and relevant dates. Having a news summary allows us to quickly perform small-to-big retrieval; finding the full article from it's brief overview. This method helps us evaluate the Language Learning Model (LLM) more effectively. Additionally, the identified entities will be used as metadata and embedded with the news articles, aiding in fine-tuning and evaluating the LLM.\n",
    "\n",
    "For more insight into this approach, check out this YouTube video by Jerry Liu, Founder of LlmamaIndex: https://youtu.be/TRjq7t2Ms5I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161c5e3-0925-4bac-b2f7-f71936c03665",
   "metadata": {},
   "source": [
    "## Config & Install Libraries\n",
    "Check if Huggingface transformers and required libraries are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50daf810-a778-4ac6-b36e-04f51dfd9e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers sentencepiece sentence-transformers datasets spacy chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb674b7-f4c8-4a30-95d3-2545b512235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa7f79-1f93-4386-96b9-1fd0aa78fbbe",
   "metadata": {},
   "source": [
    "## News Summary Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6caa0e-2aea-44e9-9379-5cc09090d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from util import utils\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba077d4f-7a01-433c-8251-ad0adab8b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a090e1-9b4f-4787-a03b-d78fe0754547",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_CONN_STRING = 'mongodb://admin:7QdQ3v0M50<>@192.168.8.166:27017/'  # os.getenv(\"MONGO_CONNECTION_STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf02214-764f-404a-9401-44a273c876c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client = MongoClient(MONGO_CONN_STRING)\n",
    "db = mongo_client.get_database(os.getenv(\"MONGO_DB_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6a3dda-cc27-4648-be12-f799485a38dc",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d3b23-3ac1-4ecf-96ed-5c8f90baa31f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "batch_date = {'$gte': '2024-04-08', '$lte': '2024-04-09'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651a5f5-0a69-41d3-bb6e-fe5a4c9017e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = json.loads(json.dumps(list(db.get_collection('raw-news').find({'created_at': batch_date})), cls=utils.CustomMongoDecoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123e5e2e-1e8d-45a9-aa4c-5b350d3a3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in news_articles:\n",
    "    article['processed_content'] = article['raw_content'].replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1a7318-5d23-46a2-8f0f-09e018f26e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article['processed_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90714437-0414-4872-bf02-8203d7fcd556",
   "metadata": {},
   "source": [
    "## Load Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60181e-0b78-42ab-8e56-7625cf061544",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/bigbird-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e956a8a-041a-40db-ab2b-d58540010d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load fine-tuned BART model for summarization\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def summarize_article(text: str):\n",
    "    # Tokenize and encode the text\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs.input_ids, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "    \n",
    "    # Decode and print the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # return summary\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05a060-9c35-4b9b-880c-80a1d191294f",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b10d8c-9895-47e5-9a3b-c183ca031c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_FIELDS = ['PERSON', 'GPE', 'NORP', 'EVENT', 'ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcddf58-cf04-4582-8237-bbb6f65aa923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "def perform_ner(text: str):\n",
    "    \n",
    "    # Load the English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract named entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c52348-6302-446c-a5a2-914a30f3414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess the named entities to select the required entity tags\n",
    "def postprocess_entities(entities):\n",
    "    processed_entities = defaultdict(set)\n",
    "    \n",
    "    for entity, label in entities:\n",
    "        if label in REQUIRED_FIELDS:\n",
    "            processed_entities[label].add(entity)\n",
    "    processed_entities = {key: list(value) for key, value in processed_entities.items()}\n",
    "    return processed_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07deb794-1e36-4fc8-8c2e-641fdb4ad483",
   "metadata": {},
   "source": [
    "## Perform Summarization and NER on News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c751ab-d550-43e7-a548-59623d86592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392d771-4c99-46e7-9933-db080ddbd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for news in news_articles:\n",
    "    summary = summarize_article(news['processed_content'])\n",
    "    entities = postprocess_entities(perform_ner(news['processed_content']))\n",
    "\n",
    "    # filter criteria\n",
    "    filter_criteria = {'_id': ObjectId(news['_id'])}\n",
    "    \n",
    "    # Define the update operation\n",
    "    update_data = {\n",
    "        '$set': {\n",
    "            'processed_news_content': news['processed_content'],\n",
    "            'news_summary': summary,\n",
    "            'entities': entities\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Update the Mongo document\n",
    "    result = db.get_collection('raw-news').update_one(filter_criteria, update_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80c5002-a6df-4a6c-9408-27040d836c51",
   "metadata": {},
   "source": [
    "## Save content and metadata on Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b98e6e-7cc8-473e-8bd4-f7fa01c80050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53662c26-4744-4a7d-9915-80f7a2dd3f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USElectionsGPT",
   "language": "python",
   "name": "uselectionsgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
