{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677b54c0-5098-4876-ae3a-cd41292612b6",
   "metadata": {},
   "source": [
    "# Mistral Quantization and Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417069b5-b003-43ed-950a-d4eac89918ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate\n",
    "!pip install -qi https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94f25a5-03c2-490a-96d7-c03606eaf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e9ba5e-9f6e-4c62-a8b1-a251781ebad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4c7ae3-4a9b-4efe-9a56-b31059d0644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236ed2e-5343-460b-b8b8-4f9cde384fa3",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "Quantization is done reduce memory footprint and perform faster inference while still retaining acceptable model performance. For this quantization, we will use bitandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06f522b-4aac-4995-8674-fb167ff722e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 4 bit compute \n",
    "use_4bit = True\n",
    "\n",
    "# compute dtype for 4-bit models\n",
    "compute_dtype = \"float16\"\n",
    "\n",
    "# quantization type\n",
    "quantization_type = 'nf4'\n",
    "\n",
    "# use double quantization\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d94a9c-289d-492b-8574-527927e276c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_compute_dtype = getattr(torch, compute_dtype)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=quantization_type,\n",
    "    bnb_4bit_compute_dtype=bnb_compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3ad33-0fd8-458b-8853-282ee3c0fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320122ca-fefe-4545-a3c8-78c2885a0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input = tokenizer.encode_plus(\"[INST] Was Vivek Ramaswamy running for president ? [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "\n",
    "generated_ids = model.generate(chat_input, \n",
    "                               max_new_tokens=1000, \n",
    "                               do_sample=True, \n",
    "                               pad_token_id=tokenizer.eos_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb87b2-97f9-4b3c-b1d0-f8660945f292",
   "metadata": {},
   "source": [
    "## Langchain And Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c6c81-6473-4140-b52c-9a2cf2000e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06188e-dde7-44cb-bd17-17f7b75ccdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc4944-e8cc-4ede-9766-d7c84c0b4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56dd0a4e-12f7-4c37-a774-7502e58faa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST]\n",
    "Instruction: You are an expert political analyst with vast knowledge of the United States electoral process. You answer questions with \n",
    "certainty and you do not hallucinate. When unsure, you politely reply that you do not have  Using this knowledge, answer the following questions.\n",
    "Here is a context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question}\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384413a-41f6-468a-b455-ac9f146cfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96537401-5143-4e91-9bbe-86a48fe2844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['context', 'question']\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91def7c-eb7a-410e-afea-3d7e178113b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm_pipeline, prompt=prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USElectionsGPT",
   "language": "python",
   "name": "uselectionsgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
