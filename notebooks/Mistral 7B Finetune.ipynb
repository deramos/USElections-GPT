{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677b54c0-5098-4876-ae3a-cd41292612b6",
   "metadata": {},
   "source": [
    "# Mistral Quantization and Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417069b5-b003-43ed-950a-d4eac89918ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate\n",
    "!pip install -qi https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f25a5-03c2-490a-96d7-c03606eaf730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9ba5e-9f6e-4c62-a8b1-a251781ebad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c7ae3-4a9b-4efe-9a56-b31059d0644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236ed2e-5343-460b-b8b8-4f9cde384fa3",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "Quantization is done reduce memory footprint and perform faster inference while still retaining acceptable model performance. For this quantization, we will use bitandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f522b-4aac-4995-8674-fb167ff722e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 4 bit compute \n",
    "use_4bit = True\n",
    "\n",
    "# compute dtype for 4-bit models\n",
    "compute_dtype = \"float16\"\n",
    "\n",
    "# quantization type\n",
    "quantization_type = 'nf4'\n",
    "\n",
    "# use double quantization\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d94a9c-289d-492b-8574-527927e276c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_compute_dtype = getattr(torch, compute_dtype)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=quantization_type,\n",
    "    bnb_4bit_compute_dtype=bnb_compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de3ad33-0fd8-458b-8853-282ee3c0fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320122ca-fefe-4545-a3c8-78c2885a0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_input = tokenizer.encode_plus(\"[INST] Was Vivek Ramaswamy running for president ? [/INST]\", return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "\n",
    "generated_ids = model.generate(chat_input, \n",
    "                               max_new_tokens=1000, \n",
    "                               do_sample=True, \n",
    "                               pad_token_id=tokenizer.eos_token_id)\n",
    "decoded = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb87b2-97f9-4b3c-b1d0-f8660945f292",
   "metadata": {},
   "source": [
    "## Langchain And Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c6c81-6473-4140-b52c-9a2cf2000e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06188e-dde7-44cb-bd17-17f7b75ccdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc4944-e8cc-4ede-9766-d7c84c0b4a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd0a4e-12f7-4c37-a774-7502e58faa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST]\n",
    "Instruction: You are an expert political analyst with vast knowledge of the United States electoral process. You answer questions with \n",
    "certainty and you do not hallucinate. When unsure, you politely reply that you do not have sufficient knowledge to answer the user question.\n",
    "You will generate new content by analysing the context supplied with each user question. Using this knowledge, answer the following questions.\n",
    "Here is the context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384413a-41f6-468a-b455-ac9f146cfbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96537401-5143-4e91-9bbe-86a48fe2844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['context', 'question']\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91def7c-eb7a-410e-afea-3d7e178113b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm_pipeline, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe27046-037a-47c9-883f-42c9cce7f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Vivek Ramaswamy, the multi-millionaire biotech entrepreneur and self-described intellectual godfather of the anti-woke movement, announced on Tuesday that he is running for president.\n",
    "“We are in the middle of a national identity crisis,” he declared in an online video launching his campaign, offering that the current political climate constituted a form of “psychological slavery.”\n",
    "Speaking straight to the camera, with an American flag draped in the background and a flag pin on his lapel, Ramaswamy framed his campaign as a broad counteroffensive to what he called the “woke left” — describing it as a threat to open speech, the free exchanging of ideas and American exceptionalism itself.\n",
    "Ramaswamy is the third high-profile candidate to declare for the presidency in 2024. Though he filed forms with the FEC declaring he would be running on the Republican side of the aisle, his announcement video made no mention of the party itself — an indication that he hopes to frame his candidacy as outside the conventional political framework.\n",
    "He has already done barnstorming in early nominating states, including Iowa, where he was well received even as some of the state’s political bigwigs professed to not having familiarity with the planks on which he was running.\n",
    "Ramaswamy made his fortune in biotech investing, but he is best known for his appearances on Fox News and for the New York Times bestselling book he has written.\n",
    "While his chances of securing the nomination are certainly long, Ramaswamy’s entry into the contest was greeted with a traditional flare from opposition Democrats. Shortly after he appeared on Fox News to elaborate on his decision to run, the Democratic National Committee sent out a statement.\n",
    "“As Vivek Ramaswamy uses Tucker Carlson’s show to announce his campaign for president, one thing is clear: The race for the MAGA base is getting messier and more crowded by the day,” it read. “Over the next few months, Republicans are guaranteed to take exceedingly extreme positions on everything from banning abortion to cutting Social Security and Medicare and we look forward to continuing to ensure every American knows just how extreme the MAGA agenda is.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a486b-3543-4ba7-bf9b-01f3d0dc52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.invoke({\"context\": context,\n",
    "                 \"question\": \"Was Vivek Ramaswamy running for president in the 2024 general election ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbecf6cd-649b-42da-bf65-8313929857f0",
   "metadata": {},
   "source": [
    "## Langchain and Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdcb0bc-07c0-4545-ab13-08a7853fe0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ddb46a-ffbf-4cbc-a7a3-50fda77d90ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import HttpClient\n",
    "chroma_client = HttpClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af328c7b-0379-4685-92e1-7866bbe4840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(client=chroma_client, \n",
    "                      embedding_function=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"), \n",
    "                      collection_name='us-election-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859468b1-b944-4195-a0dc-dbe1383f4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a91bcf-5050-41ac-975c-e263f77cef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is Donald Trump up to lately?'\n",
    "query_embedding = tokenizer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dba8d0-4c81-4cff-a366-0c8f8b4298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead48340-496b-4a94-8709-780290278982",
   "metadata": {},
   "source": [
    "## Retriever with Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3757ec-ca76-424f-94b7-5e5f9abc344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13bbbeb-53b3-4516-b317-9a2b9604b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chat History retrieval prompt\n",
    "chat_history_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc70ee7-bedc-43f2-858f-a53eec00a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_context_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', chat_history_system_prompt),\n",
    "     MessagesPlaceholder('chat_history'),\n",
    "     ('human', '{input}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983b4e9-8437-490e-ab50-ef4ea2c9706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(llm_pipeline, retriever, chat_history_context_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c943f-7016-4118-804b-5b36c0c257d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## New question/answer prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', prompt_template),\n",
    "     MessagesPlaceholder('chat_history'),\n",
    "     ('human', '{input}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c446f8b-e8e0-4f22-8b50-36748c55219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm_pipeline, chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914d6fd-f41d-41a0-9904-e7dd75ff67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92556157-0d03-4737-914d-e1a5163e9fd3",
   "metadata": {},
   "source": [
    "### Redis Chat History\n",
    "integrate Redis to store chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a11f61-4237-45bb-8399-554b11e54bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_URL = \"redis://localhost:6379/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5c046-1dd6-42e3-84b6-b1ab4461072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cb989-bbf6-4895-930e-7bc264d44046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id, url=REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fa47a-c407-4ed0-a506-8d3247a28a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_llm = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21340e83-5091-403d-bbd5-1571d43e3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_llm.invoke(\n",
    "    {\"input\": \"Did Vivek Ramaswamy run for president in the 2024 general election ?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USElectionsGPT",
   "language": "python",
   "name": "uselectionsgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
