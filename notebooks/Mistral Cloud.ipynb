{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fd721-68cd-44b1-9d48-ae6ffbf73c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from chromadb import HttpClient\n",
    "from langchain.vectorstores import Chroma\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abb303-939c-4616-b501-6368e4de6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cefb244-c357-4b75-a1fd-3aaecf914427",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fc907-a5f2-4d50-8eb0-f47fc10dad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaEmbeddingsAdapter(Embeddings):\n",
    "    def __init__(self, ef: EmbeddingFunction):\n",
    "        self.ef = ef\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.ef(texts)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.ef([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad16e65-8f73-40de-b9f8-283d9e8b5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = ChromaEmbeddingsAdapter(embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885bbc9-1243-45f4-8996-c19aa6c1ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = HttpClient(os.getenv('CHROMA_DB_URL'))\n",
    "vector_store = Chroma(client=chroma_client, collection_name=os.getenv('DB_NAME'), \n",
    "                      embedding_function=embedding_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c7549-0854-47dd-92ba-f921989e9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatMistralAI(mistral_api_key=os.getenv('MISTRAL_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86252d6-e176-4887-bc1f-5ac25fb10958",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_prompt = \"\"\"\n",
    "    Given a chat history and the latest user question which might reference context in the chat history, \n",
    "    formulate a standalone question which can be understood without the chat history. Do NOT answer \n",
    "    the question, just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "user_input_prompt = \"\"\"\n",
    "    ### [INST]\n",
    "    Instruction: You are an expert political analyst with vast knowledge of the United States electoral process.\n",
    "    You answer questions with certainty and you do not hallucinate. When unsure, you politely reply that you do \n",
    "    not have sufficient knowledge to answer the user question. You will generate new content by analysing the \n",
    "    context supplied with each user question. When your previous knowledge is capable of answering the questions, or when the \n",
    "    supplied context isn't enough to do so, you can default to previous knowledge. Using this instructions, answer the \n",
    "    following questions. Here is the supplied context:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528260a2-93d6-4c3b-953a-74dfbce53d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get chromadb retriever from vector store\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0343a-0719-46f4-8f92-608dd8d1fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id, url=f\"rediss://default:AdqMAAIncDE0MjU5OTI2NWM1MzM0MTMzODY2MTA0NzAwY2VjMDU3OHAxNTU5NDg@capital-garfish-55948.upstash.io:6379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb0aff-77f7-4a81-a42a-4bd7ecfc3559",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_context_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', chat_history_prompt),\n",
    "    MessagesPlaceholder('chat_history'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "# create history aware retriever using chromadb retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model,\n",
    "    retriever,\n",
    "    chat_history_context_prompt\n",
    ")\n",
    "\n",
    "# New question/answer prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system', user_input_prompt),\n",
    "      MessagesPlaceholder('chat_history'),\n",
    "     ('human', '{input}')\n",
    "     ]\n",
    ")\n",
    "\n",
    "# create document chain\n",
    "question_answer_chain = create_stuff_documents_chain(model, chat_prompt)\n",
    "\n",
    "# create rag_chain using system and user prompts\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "# create runnable llm with message history\n",
    "rag_chain_llm = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbde6b9-85f9-42b2-bc0d-2bbae73084ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUERY = \"\"\"\n",
    "    What's the latest in Texas?\n",
    "\"\"\"\n",
    "query_embeddings = embedding_fn.embed_query(TEST_QUERY)\n",
    "np.array([query_embeddings]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a1f66-db6a-4781-851e-6362801d65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_collection(os.getenv('DB_NAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ba731-6d71-42e6-a6d4-f60e73ab7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", collection.count(), \"items in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294da90c-e45c-421e-9a22-81a3dfa8a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed36fe-a18c-489d-919f-347717496770",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00194d93-1109-4ead-9c52-01c3e51e2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What's the latest with Donald Trump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d392a12-a748-48bf-b78f-3469051f1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = rag_chain_llm.invoke(\n",
    "    {\"input\": message},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2cb4a-cfd1-41e1-936c-aae9e0c91af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(chat_response['answer'].split('\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc634898-48d4-486e-b9f2-f637d9fbaacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3dade4-6992-4c75-8c6e-cb88efcd90f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USElectionsGPT",
   "language": "python",
   "name": "uselectionsgpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
